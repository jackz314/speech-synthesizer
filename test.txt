Hello, this is a test message. Hopefully this will work.

Balance of power: the social-ethical aspect of data mining
L. Tuovinen and J. Roning
Intelligent Systems Group, Department of Electrical and Information Engineering P.O. BOX 4500, FIN-90014 University of Oulu, Finland
{lauri.tuovinen, juha.roning}@ee.oulu.fi


Abstract

Data mining technology offers a powerful instrument for acquiring knowledge. Knowledge concerning individuals sometimes contains details considered too sensitive to be disclosed, which has caused researchers to take an interest in developing data mining methods that produce high quality results while guaranteeing the privacy of the people involved. Research on the ethics of data mining has, however, largely neglected the possibility of using the technology in ways that are not merely ethically sound but positively commendable. The purpose of this paper is to bring the situation closer to balance. The social-ethical aspect of data mining is viewed from both perspectives: what makes it a threat, on the one hand, and what makes it an opportunity, on the other. We find that there are justified concerns over the effects on civil liberties of certain uses of data mining, but also that there are many benefits to be reaped in such fields as crime investigation and medical diagnostics. Being aware of the whole ethical equation, not just one side of it, will help researchers and practitioners of data mining analyse the implications of their work more accurately.


Keywords

data mining, knowledge discovery, computer ethics, privacy preservation, law enforcement, medical science


INTRODUCTION

'Knowledge is power', states the popular slogan coined by the 16th-century British philosopher Francis Bacon, a tireless promoter of the advancement of scientific research. The same identity was recognised, albeit in a somewhat more cynical sense, by Friedrich Nietzsche, who claimed that our desire for knowledge is really a desire for power. Data mining, a technique for extracting knowledge from collections of data too vast or complicated for unassisted human reasoning to cope with, can therefore be viewed as an instrument for acquiring power. A wisdom much older than Bacon's tells us that power has the tendency to corrupt, and although we can be hopeful that it does not inevitably do so, it cannot be denied that a tool like data mining, when applied to human beings, must have certain ethical implications.
The ethical dilemma of data mining technology stems from the fact that when data representing qualities and activities of individuals is mined, the result may be the disclosure of information that the subjects of the data never intended to be disclosed, even though the data may have been gathered with their consent. This fundamental problem of privacy violation is further amplified if the data is insecure or incorrect (Wahlstrom and Roddick, 2000). In the worst case scenario the source data or the discovered knowledge is deliberately corrupted or otherwise used to attack an individual or group. The knowledge discovery and computer ethics communities have thus been compelled to direct their attention toward establishing ethical standards for data mining and technical means for ensuring that the standards are not (advertently or inadvertently) disobeyed.
Data mining is no longer merely an academic exercise or a corporate buzzword but a matter of public interest. In fact, public concern for the morally questionable uses of data mining has elicited a response from the ACM SIGKDD Executive Committee in the form of an open letter (Kim et al., 2003). The authors of the letter wish to make it clear that specific undertakings employing data mining technology should not be regarded as synonymous with data mining in general. The fact that such a letter was considered necessary by some of the most prominent figures in data mining research and development stresses the importance of keeping everyone, decision makers and the general public alike, correctly informed of what data mining is and what it is not.
Distinguishing the theory of data mining from specific applications of data mining is relatively easy. A much more difficult line to draw is the one that separates the domain of governmental control from the liberties of citizens, the very source of the concerns addressed in the SIGKDD
directors' letter. Previous studies on the ethics of data mining have largely disregarded one side of the issue, concentrating on how data mining can be prevented from becoming a handy tool for a metaphorical Big Brother. Such research is negative ethics in the sense that its objective is to discourage uses of data mining technology deemed harmful. There is, however, another side to the equation: if data mining can be used, for example, to fight crime, is it unreasonable to call for a discussion on relaxing individuals' demands for privacy for the benefit of all? How the equation should be balanced stands open to debate, but we cannot afford to adopt a naive absolutist position and pretend that it does not exist. Besides, the positive-ethical side also includes applications, such as medical diagnostics, to which it is very difficult to object in the context of any widely accepted value system.
The present global antiterrorism campaign has heated up the privacy vs. security debate, supplying both factions with fresh ammo: one side argues that we must give up some of our liberty in order to ensure our survival, while the other maintains that now more than ever we must hold on to our liberty lest the terrorists get what they want. Given all this, there seems to be a need for a survey of the social-ethical issues of data mining that places an equal weight on positive and negative ethics. In this paper we provide such a survey while trying to be as value neutral as it is possible to be when discussing the subject of applied ethics. We also burrow some way into the history of ideologies relevant to our points of interest, thus establishing a broader context for our topics in the form of philosophical background.
The second section of this paper, 'Analysing the ethics of computing', looks at ways of studying the ethical implications of computing technologies and discusses their application in the context of data mining. The third section, 'The case against data mining', explains in detail the ethical problems associated with data mining and reviews research aimed at mitigating those problems.
The fourth section, 'The other side of the story', presents arguments for data mining as a positive force and reviews reported applications that can be considered constructively ethical. The conclusions of the paper are presented in the fifth section.


ANALYSING THE ETHICS OF COMPUTING

An entire chapter of (Fogg, 2003), a textbook on persuasive technology, is dedicated to ethical issues. Although the overall topic of the book, the use of computers to influence opinions and behaviour, is conceptually rather distant from data mining, many of the guidelines presented in the book are useful in assessing the ethical implications of any technology. In this section we explore how these guidelines can help us analyse the ethical aspect of data mining technology.
One approach proposed in the book is to consider the intentions, methods and outcomes of a given application of persuasive technology. The same approach is directly applicable to data mining as well. In other words, we can ask the following three questions: Does the data miner have malevolent motives? Are the methods employed harmful to the subjects? Is it conceivable that the knowledge discovered could hurt them in some way? If the answer to any of these is any other than an unconditional 'no', there may be an ethical problem involved. It is important to consider each of these areas separately, because they are independent in the sense that even if any two of them are ethically sound, the remaining one may still compromise the morality of the whole venture.
It is relatively easy to come up with examples of both ethical and unethical intentions, methods and outcomes of data mining. For instance, if the data miner's intent is to discover sensitive knowledge that can be used to coerce data subjects into doing things they would otherwise not do, there is clearly an ethical problem involved; if the intent is to apply image mining techniques on X-ray photographs in order to identify possible tumours, the miner is to be commended. If data is gathered by tracking the activities of individuals candidly and without their consent, there is an ethical problem with methodology; if the data is voluntarily provided by the individuals themselves with full understanding of the implications, no such problem exists. Finally, if the mining results include knowledge that was never meant to be disclosed to anyone, they are ethically problematic, while outcomes that are suitably restricted, accurate and secure only pose a problem when used with questionable intent.
The book also proposes a seven-step process for identifying ethical concerns related to persuasive technology products. Again, the items of the list are generic enough to be useful in various contexts, including data mining. The steps are as follows:
1. List all of the stakeholders.
2. List what each stakeholder has to gain.
3. List what each stakeholder has to lose.
4. Evaluate which stakeholder has the most to gain.
5. Evaluate which stakeholder has the most to lose.
6. Determine ethics by examining gains and losses in terms of values.
7. Acknowledge the values and assumptions you bring to your analysis.
Applying the process requires that the analyst be able to quantify his or her values. It can therefore be thought of as a form of utilitarianism, which is a common name for ethical theories based on the notion that the moral quality of an action can be defined as the sum total of its desirable consequences, i.e. its utility. Converting ethical values into something that can be added and subtracted is counterintuitive, and the basis of utilitarian philosophy in general is often challenged, but a systematic analysis requires a systematic methodology. The need to rely on imprecise and seemingly arbitrary personal judgments cannot be completely eliminated from ethics, but they can be made more disciplined by using them within a framework that regulates the way they are exercised. This is what makes the seven-step process a useful tool, especially when coupled with the naturalistic notion that ethical values can (in some cases, at least) be identified with the physical, psychological and social needs of people. This theme will be explored a bit further in the fourth section.
The last step deserves extra attention, as it is perhaps the easiest to forget and the most difficult to carry out comprehensively. In the choices one makes in one's everyday life the underlying ethical principles remain largely unconscious and unquestioned, because the cognitive strain of carefully judging the soundness of every value with every decision would be much too high. However, when one judges the actions of others, one effectively imposes one's own value system on someone who may not agree with it at all and has no obligation to, so it is not constructive to treat one's values as absolutes, which is what one essentially does every time one makes an automatic or 'obvious'
choice concerning only oneself. The values on which the judgment is based may not be all that obvious to others, so it is important to identify them and to keep in mind that even the most brilliant and perceptive analysis is subject to reservations. Most of the time we rely, mostly successfully, on our ethical intuitions as the only practical alternative, but to believe as Moore (1903) did that such intuitions can yield us solid facts about morality is not very helpful; for IT professionals to establish and enforce ethical guidelines for their work a less subjective approach is necessary, even though an intuitionist like Moore would condemn it as naturalistic and therefore fallacious.
Yet another important point raised in (Fogg, 2003) is that perspective makes a world of difference. Advertising, for instance, is something that not many would deem unethical as such, but in a wider context it can be argued to promote increased consumption and thus, ultimately, waste of natural resources - probably not the intent of the advertisers, but a known effect nevertheless.
Correspondingly, data mining can be used in niche marketing to match products and marketing strategies with target groups (Chou et al., 2000; Gersten et al., 2000; Russell and Lodwick, 1999), which the customers might even praise as improved service but which, on a large scale and over a large timeframe, may harm the environment (although it can also be argued that environment is conserved because less paper is required for direct-mail advertisements). Another potential problem with advertising is that it can reach consumer groups that are particularly vulnerable to persuasion, classically children. In such cases the result of ethical analysis depends not only on intentions, methods, outcomes and underlying values but also on to what extent the data miner is held responsible for the consequences of numerous similar mining efforts, to which an individual miner only contributes a little.


THE CASE AGAINST DATA MINING

The principles of modern liberal democracy were grounded in the works of 17th-to 19th-century European political thinkers like Thomas Hobbes (1651), John Locke (1698) and John Stuart Mill (1859). The views they expressed are rooted in the notion of a social contract. A social contract is an implicit collective agreement, the participants of which give up some of their individuality in order to set up an organised society to replace the so called natural condition, in which there is no rule of law. Multiple variations of the exact formulation of these central concepts exist, but the basic idea is that in the natural condition each person is entitled to do whatever he or she wishes, whereas in a society some of these rights (e.g. the right to use violent force) are turned in exclusively to the government in exchange for protection. The main point of the social contract
hypothesis is that a state is regarded as a purely artificial construct, in contrast to the view held by Aristotle and many other philosophers of the ancient era that living in a society is the natural condition of humanity.
When viewed as the result of a contract, a state can be thought of as an instrument whose legitimacy is wholly dependent on its ability to serve its owner (i.e. the citizens that constitute the state). From this perspective the only thing that has value in itself is the individual; the value of a society comes from its functionality, i.e. the benefit it can bring to the participating individuals.
Unless the benefit is overwhelming, individuals are typically very reluctant to give up such liberties as freedom of thought, speech and religion, so the conception of the origin and role of government described above becomes an argument in favour of tolerance for the peaceful coexistence of different ways of life within a society. Mill argued that guaranteeing such tolerance is, to a state, not only an obligation but also a strength. A well known and interesting contemporary argument is John Rawls' (1971) 'veil of ignorance': if individuals were to design a social contract before they knew anything of the position they would occupy in life, the resulting society would be one of equality and toleration.
Although brief and simplistic, this review of post-medieval western political philosophy should suffice to illustrate the length of the tradition of demanding the preservation of civil liberties. This deep-rooted tradition is probably a significant reason to why members of liberal societies tend to react passionately when they believe their liberties to be compromised. One example of this, the opposition to data mining, was presented in the introduction. Privacy, in particular, is something of a universal psychological need of human beings, although what exactly is perceived as private varies from culture to culture and from individual to individual (Brown, 1990). While criticism of data mining in general may be based on a misunderstanding, there is no doubt that data mining techniques can be used in ways that many people as individual citizens find unacceptable. The two main applications of data mining that may in some cases be regarded as civil rights violations are surveillance (or screening) and profiling.


Data mining for surveillance

Privacy is, by definition, a personal thing. What this implies is that if data gathered from individuals is not associated with any personal information that could be used to identify specific individuals, it is difficult to think of a scenario in which privacy (or any other civil liberty for that matter) could be violated by just mining the data. The situation is very different when data mining techniques are used for screening, i.e. to pick out individuals meeting certain conditions. The general idea is that individuals who pass the screen are of particular interest in some way, whereas those blocked by the screen can be safely ignored. However, the possibility of false positives (an individual not meant to be picked is picked) and false negatives (an individual meant to be picked is not picked) is never out of the question. This is a technical problem from the point of view of those implementing the screen, but a potential moral problem from the point of view of those being screened. False positives are particularly problematic from the ethical perspective, as they may result in some defensive action being directed against people completely undeserving of such a treatment; for instance, a loan application from a financially stable client might be rejected.
We see now that there is a strong ethical motive besides the practical one for the development of more accurate screening algorithms. The same goes for data collection and cleansing methods, because mining results are only as reliable as the source data. Different stakeholders often disagree on the appropriate degree of tolerance for false positives and negatives and thus on the appropriate focus of development, generating more ethical debate. All in all, though, the elimination of false results is in everyone's interest, so it is likely that the problem of inaccurate results will largely correct itself over time. However, if the justification of the whole screening process is questionable, the problem is much more complicated. This is because the heart of the problem is not the technology but the intentions of the people applying the technology. It is possible to equip data mining and warehousing tools with features designed to protect the rights of data subjects, but their effectiveness ultimately depends on the willingness of users to be bound by them. Given a suitable set of tools and data, nothing short of criminal law can prevent the tools from being used to uncover sensitive personal details. Besides, as Lyon (2001) points out, modern surveillance is powerfully shaped by social change as well as enabling technology: surveillance information flows ever more freely across organisational boundaries, making it more and more difficult to monitor and control who gets to access the information and how it is used. The possibility of information theft is also a risk to be accounted for.
Yet further factors come into play when we consider data mining projects sanctified by state legislation and carried out by governmental organisations. The TIA and CAPPS II projects mentioned in (Kim et al., 2003) are examples of such projects. Instead of legality, which is still a comparably straightforward concept, we are then dealing with the profound ethical-political question of how much control a government has the right to exercise over its subjects. Even a remotely comprehensive discussion of this question would be much too wide for the scope of this paper, but we shall return to it briefly in the next section. In any event, we can see here an example of how far the social implications of a technology may stretch - even if the technology is completely immaterial, as opposed to, say, nuclear power.
Descending from the windy peaks of political debate back closer to the grassroots level of software engineering, we can consider the simplest case, in which the data miner is willing to protect the rights of the data subjects and to adopt any available technical means to assist in the task. Most importantly, the miner should be prepared to listen to the wishes of the subjects themselves, as they are the ones who ultimately decide whether their privacy has been violated.
This is not trivial, because a nonpractitioner may not be able to adequately appreciate the power of the methods being applied. Furthermore, providing only the options of disclosure and nondisclosure may prove too inflexible. A technique for restricting the degree of personal detail that a data mining algorithm can uncover is therefore required. Some of the proposed techniques are reviewed in the subsection 'Privacy preservation techniques'.


Data mining for profiling

Profiling is, in a sense, the exact opposite of surveillance. Instead of starting with a large group of people and looking for individuals meeting given conditions, a profiling operation starts with a given individual and looks for any information that might be useful in constructing an accurate characterisation of the individual. Like surveillance, profiling is nothing new; it has been used by crime investigators and employers (among others) long before the emergence of advanced computing technology. However, again like surveillance, profiling can be made considerably more efficient by using data mining techniques.
An example of profiling in the context of intelligence is given in (Thuraisingham and Ceruti, 2000). The paper discusses, among other topics, the possibility of using text mining to obtain profiles of the activities of faction leaders: who they are collaborating with, who they are operating against et cetera. This would be achieved by analysing news stories, which are representative of the kind of data typically used for profiling in that it comes from a variety of sources and is largely unstructured. The study concludes that current mining utilities are inadequate for the proposed task, but the world of information technology evolves rapidly, and indeed the conclusion is half a decade old as we speak. The concept of this profiling example is, in turn, closely related to another topic of interest of ours, the use of data mining to support law enforcement, which is discussed in the next section.
Another way in which profiling radically differs from surveillance is that an invasion on the subject's privacy is almost implied, since the data miner naturally wishes to discover as many relevant facts as possible in order to make whatever decision is being made as informed as possible.
The subject cannot escape identification, because a profile of an unidentified subject is of no use to anyone - unless it can be used to track down the subject. Any information that would work for the subject's benefit could probably be acquired simply by querying the subject, so we can regard the profiler as a kind of adversary to the person being profiled. The ethical nature of the profiling operation then becomes a matter of on whose terms the profiling is carried out.
Let us consider an example of an employer trying to decide whether or not to hire a given applicant and wishing to use data mining to support the decision. In the most benign case, and with suitable data at hand, the employer might simply try to verify some achievements listed in the applicant's resume (e.g. 'Is it true that the applicant was active within his student union?').
However, with today's text mining capabilities and the abundance of information available through public access networks, the employer might be tempted to take it a step further and look around for facts that are not strictly relevant to the applicant's ability to perform the job but will, nevertheless, affect the employer's personal view of the applicant - say, some relatively innocent, but compromising when taken out of context, party photographs from those very same student union years. What's worse, the 'facts', if discovered from a body of data of uneven reliability such as the World Wide Web, may be misleading or downright false. While discrimination on irrelevant
grounds is primarily an attitude problem, it is nevertheless a smaller problem if the availability of information can be controlled.
In fact, the above example reflects a much wider issue that is present also in the context of screening applications. It is not just the inferred knowledge that may need to be restricted: the pruning process should begin at source data selection. Many people are somewhat sensitive of such attributes as ethnicity and religion, and they are understandably angered when they feel that they are being treated as suspicious for coming from a particular country or belonging to a particular church.
Some believe that if statistical probability supports the use of such group membership information then it is plain common sense to use it, while others deem it wrong to categorise people as more likely to commit crimes based on qualities that are not in themselves related to criminal conduct at all. This is a major aspect of the ongoing debate fuelled by the war on terrorism; the severity of the alleged crimes is a powerful argument, but so is the severity of the consequences to the wrongfully accused, especially now that countries such as the United States and Great Britain have altered their legislation to lessen the rights of terrorism suspects.


Privacy preservation techniques

One possible method for protecting the privacy of individuals subjected to data mining is to request from them not only their permission to use their personal information but also their view, suitably quantified, of how sensitive each data item is. The sensitivity of a piece of information derived from the original data through data mining could then be computed from the sensitivity values of the data items involved in the reasoning, and the results would be presented to the data miner only if the overall sensitivity falls short of a predefined threshold. This approach has the advantage that it takes into account the fact that different people feel differently about their privacy: what one person would allow to be revealed to others without hesitation someone else might guard as an intimate secret. (Wahlstrom and Roddick, 2000)
Using individualised sensitivity factors is an example of what is more generally known as rule hiding or rule confusion. Rule confusion results in weaker rules and better privacy protection, a necessary tradeoff if sensitive data is to be guarded from exposure. The task is somewhat complex, but heuristics have been devised for a number of rule types including classification, clustering and association rules. Such heuristics can be based on, for instance, perturbation (some of the data is altered) or blocking (access to some of the data is denied). (Verykios et al., 2004) Another approach is to perturb the raw data and then reconstruct its value distributions at a higher level of abstraction. This should be done so that inferring confidential values becomes impossible but the quality of results does not suffer so much that they are rendered useless. Reconstruction-based techniques have been developed for both numerical and categorical data. Suitable perturbation of raw data can be achieved, for example, through randomisation. (Verykios et al., 2004)
Research on privacy preserving data mining techniques remains vivid. One of the most recent concerns is the protection of privacy in various personalisation and recommendation systems based on data mining (Ali and van Stam, 2004; Mor and Minguillon, 2004; Nasraoui and Petenes, 2004).
A classification scheme and state-of-the-art review of privacy preservation methods is presented in (Verykios et al., 2004). There is also a paper by Jensen et al. (2003) calling for more accurate assessment of the privacy implications of data mining and proposing a tentative model for such assessment, which they claim can help in tuning mining systems so that they better meet the demands of both officials (for results that can be used to enforce order in society) and the public (for protection of civil liberties).
THE OTHER SIDE OF THE STORY
The obvious (and frequent) counterargument to claims for privacy is 'What do you need privacy for unless you have something to hide?' The counter-counterargument is ideological: privacy is a fundamental right that requires no rational justification. In other words, it is not the job of an individual to explain why he or she wants privacy; it is the job of the government to explain why the individual should not have it. This general attitude to issues concerning the relationship between government and individual is known as libertarianism, the opposite of which is authoritarianism.
Actual states typically exhibit both authoritarian and libertarian characteristics, so that, for example, the police is permitted to tap the phone of a suspected criminal, but only with a court order based on substantial evidence. While quantitative issues are always debatable, few are so radical as to
categorically refuse the necessity and justification of such forms of surveillance and control; indeed, a state that exerts no surveillance over its citizens can hardly be called a state at all.
In the previous section we examined data mining from a libertarian perspective, viewing it as a potential instrument of morally questionable activities. There is, however, no doubt that data mining can be used in a way that is ethically sound. What, then, does 'ethical' mean in this context? It could be defined simply as 'not unethical', in which case the definition would encompass not only that which promotes good but also that which makes no difference one way or the other. Exploring the morally indifferent uses of data mining would contribute little to the discussion at hand, so we shall restrict our use of the word 'ethical' to cover only such cases where a conscious effort is made to do something that is morally commendable by some generally accepted standard. This is what we have referred to above as positive ethics.
The next question to be answered is a much more difficult one: what is good? In Europe alone this problem has been scrutinised by several of the most brilliant minds of all times starting with Socrates and Plato, and Asian philosophers have been equally productive on the subject. From all these attempts at a universal definition of the concept of moral good no final answer has emerged, which makes it tempting to concede that no such answer is possible. Most certainly such an answer cannot be provided in this paper, but there is a lot of fruitful discussion to be had even if we do not strive quite that high. Any two given people are unlikely to agree with each other on every single ethical issue, but still they are likely to find a significant degree of consensus on the importance of such fundamental values as health and happiness, even if they come from different cultures. This consensus is reflected in numerous international agreements, most prominently the UN declaration of human rights.
Ethics is the study of good life, and as Kekes (1993) points out, there are certain minimal conditions - necessary but not sufficient, in mathematical terms - to all good lives everywhere.
While we cannot compile a comprehensive list of the components of a good life that everyone will accept, we can seek partial answers within those application domains of data mining that can help us fulfil conditions we know to be essential to good living. We shall discuss two of these domains, law enforcement and medicine, in detail.


Data mining in law enforcement

In our search for positively ethical applications of data mining, law enforcement appears to be a good starting point. This is not only because it is fairly easy to justify, but also because it gives us an authoritarian counterpoint to the themes discussed in the previous section. Whether or not laws are always good and whether or not obeying them is always the right thing to do is another debate that cannot receive here the attention it deserves; what we can do is consider the notion that obeying laws must be ethical to some extent because the laws have been designed to preserve order in society and therefore, ultimately, to increase the security of individuals. To feel safe is a very natural desire of human beings, so for those unwilling to constantly fight for their survival it is only rational to cooperate with others in order to bring about laws that protect their property (in the broad sense, which covers the body of a person as well as his or her external possessions). In a way, living by the laws can be seen as a form of honesty - by honouring a law a person honours the social contract behind the law, an agreement to which every member of a society is implicitly bound by his or her choice to live in that society.
Crime related data mining research efforts typically fall into one of three major categories. One of these involves collecting characteristics (location, time, offender and victim profiles etc.) of similar crimes and mining them for patterns to aid investigation, prosecution, prediction or prevention. The types of crime that can be tackled with the help of computer assisted pattern analysis include theft, burglary and robbery (Brown and Oxford, 2001; Lin and Brown, 2002; Oatley and Ewart, 2003; Underson, 2002) as well as financial (Donoho, 2004; Zhang et al., 2003) and violent (Adderley and Musgrove, 2001; Kangas et al., 1999) crimes.
Another major body of research uses data mining techniques to expose attempts at fraud. In this case the organisation carrying out the mining is usually not the police but the intended victim.
Potential beneficiaries of fraud detection include credit card companies (Brause et al., 1999; Syeda et al., 2002), insurance companies (Viaene et al., 2004), telephone companies (Cox et al., 1997), health care providers (Forgionne et al., 1999; Sokol, 1998), tax collectors (Bonchi et al., 1999) and customs (Shao et al., 2002). A credit card company, for instance, may mine purchases made on a card for outliers, i.e. transactions that are unusual for the holder of the card and therefore suspicious.
The third category of studies deals with a type of crime novel to the information age: breaking into an information system. Administrators can detect intrusions by unauthorised users by applying data mining algorithms to usage logs and network traffic. Various modelling techniques including classification (Nong and Xiangyang, 2000), clustering (Oh and Lee, 2003) and association rules (Abraham and de Vel, 2002) are useful for identifying suspicious activities. In (Han and Cho, 2003) it is suggested that for optimal efficiency an intrusion detection system integrating multiple models should be used.
Now is probably a good time to be reminded that although the preceding discussion has been largely concerned with privacy, the dilemma here is also one of justice, namely what Lyon (2001) calls actuarial justice. As opposed to the traditional view of justice, in which offenders are sought out, arrested and brought to trial, actuarial justice strives to predict criminal acts and to prevent them from taking place, by force if necessary. An extreme, fictitious yet thought provoking, example is the 1956 short story 'The Minority Report' by Philip K. Dick, in which the protagonist faces charges of 'pre-murder' and must clear his name. Such pre-emptive power is an attractive notion - who has not dreamed of a society free of crime? - but the very concrete consequences of law enforcement decisions make accuracy a critical issue. How many false positives are to be tolerated when the physical immunity, not just privacy, of data subjects is at stake? Computers are not a prerequisite to policing errors, of course, but there is a risk that the apparent massive efficiency of data mining technology blinds its users to its flaws. Computing power can greatly enhance the scale of achievable results, but the scale of potential errors is correspondingly magnified in the process. The point of all this is that while there clearly lies a lot of potential in data mining as a crime fighting tool, that potential is not an excuse to neglect weighing gains against costs. On the other hand, we must neither forget that there is an ethical cost, the failure to prevent a crime, associated with false negatives as well.


Data mining in medicine

Giving aid to a fellow human being in need is generally considered an important moral obligation -
after all, the most fundamental requirement of good life must certainly be life itself (Kekes, 1993).
The principle is embedded in the golden rule of treating others the way one wishes oneself to be treated, common to many religions. It is also implicit in Immanuel Kant's famous imperative to always treat another person not merely as a means but also as an end. This, along with the fact that there is an abundance of medical data that may yield valuable knowledge to physicians through computational analysis, makes the practice of medicine an obvious example of an ethical application of data mining that, unlike criminological applications, poses no threat to anyone's privacy. Not that medicine in general is devoid of ethical issues, mind you - physicians and policy makers possess a great deal of control over patients, and their views of what is best for a patient do not always agree with the patient's own view (Hayry, 1998).
Like so many other fields of human expertise, medicine is not only a science but also an art. A doctor has the collective knowledge of the international life science community at his or her disposal, but he or she also often needs to rely on intuition honed by years of experience in the field. The intuition of a skilled professional is often correct, but it necessarily involves cognitive processes taking place at the subconscious level, so the line of reasoning is very difficult to formalise as a computable algorithm. The human brain is a powerful instrument for making associations and recognising patterns, so a physician may identify the condition of a given patient with great confidence but still be unable to express the diagnostic decision process in terms of simple rules that a computer can follow. However, a certain type of algorithm can generate such rules for itself when presented with a sufficient quantity of training data that has been pre-analysed by human experts. Modern computers can efficiently process much larger batches of data than is practical for humans, which makes them a valuable tool for reducing uncertainty in the work of specialists who do not have the option of delaying a decision until it is definitely known to be correct.
A classic example of a task in which humans have traditionally outperformed computers is the interpretation of images. Most of us observe and comprehend the world primarily through what we see, and the flow of visual data to the brain is continuous, so we need to have a very sophisticated neurobiological system for identifying various objects in our field of vision and generating appropriate responses to them. Computers do not possess such a natural inclination towards anything except pure arithmetic, into which our inherent image processing capabilities do not readily translate. That said, there have been many advances in the field of digital image analysis
that bring benefits to medical science. Physicians use images obtained through various techniques such as X-ray and magnetic resonance imaging (MRI) to help them diagnose patients, and numerous algorithms exist to reduce the cognitive burden of interpreting these images, for example through content based retrieval (Nielsen and Hansen, 2004; Petrakis et al., 2002) or automatic segmentation (Richard et al., 2004; Zhang and Chen, 2004).
Besides medical image processing in general, certain classes of diseases stand out as areas of medicine where there have been several successful applications of data mining. One of these is comprised by physically or mentally impairing conditions usually associated with old age such as arthritis and Alzheimer's disease. Data mining can help doctors come up with an early diagnosis, which is important because these conditions are known to be easier to cope with when identified before the symptoms become serious (Wyns et al., 2004; Zaffalon et al., 2003). It can also assist them in developing effective therapeutic approaches (Walker et al., 2004).
The diagnosis and classification of cancer and tumours is another line of research that has attracted considerable attention from the medical AI community. Also with these it is often the case that early detection is a critical prerequisite to successful treatment; with ovarian cancer, for instance, data mining as a diagnostic tool may help decrease mortality rates (Li et al., 2004; Sorace and Zhan, 2003). Known tumours can be analysed with data mining techniques in order to determine their type (Lukas et al., 2004) or to predict their likelihood of malignancy (Antal et al., 2003). Computational analysis can also be of assistance in selecting cancer patients for clinical trials (Seroussi and Bouaud, 2003).


The subtle nature of ethics

Interestingly, there is a recent study by Wang et al. (2004) on using granular computing to protect the privacy of individuals whose personal information has been stored in medical databases, thus combining the twin goals of supporting medicine and preserving privacy. Another amusing fact is that data mining can be used to combat information crime, thus making data safer and reducing the risks of data mining itself. These may seem like rather trivial coincidences, but in a small way they illustrate the fact that ethical issues are ubiquitous - sufficiently so to turn up more than one at a time without conscious intent on the part of authors. They are also intensely interconnected: the rights of individuals cannot be discussed without discussing also the rights of authorities, which in turn cannot be done if we omit to consider the ultimate source of legitimate power, et cetera.
The bottom line is that ethics is something entirely human. On the one hand this means that there can be no ethics without humans, and ethics as a discipline has many of the qualities of the human mind: it is full of fuzziness, informality and free association. In the face of cultural relativism and with the weight of religious arguments radically diminished since medieval times, the academic study of ethics has all but lost its normative quality. Rather than trying to establish universal criteria for good life and proper conduct, today's ethicists mostly concentrate on working out the semantics of ethical concepts, an activity that might more accurately be termed metaethics.
The equation works both ways, however: just as there can be no ethics without humans, there can be no humans without ethics either. What this means is that there is an ethical aspect, inherent and inextricable, to everything humans do. Therefore, even though ethical principles cannot be proven correct, and even if correctness is not even a meaningful concept in this context, we cannot escape having them and being ruled by them. Although laws usually reflect some sort of ethical consensus within a society, they are not meant to be, and cannot be, a substitute for ethics. There will always be conflicts that law cannot solve, so there has to be a social framework that allows people to settle their differences amongst themselves - a moral code.
We do not wish to impose our personal ethical principles upon others; we simply suggest that since the presence of ethical questions cannot be avoided, they should be embraced rather than ignored. There is a very pragmatic motive involved in addition to the ideological one, since the success of an information technology product depends heavily on its perceived ethical acceptability.
Although one can perhaps never know with absolute certainty that one is doing the right thing, one is always on firmer ground when one has given the matter due consideration. With our discussions we hope to make a contribution for the increased ethical awareness of the data mining community.


CONCLUSIONS

In this paper the social and ethical implications of data mining technology were discussed. The most widely studied of these implications is the threat to privacy present when personal information is subjected to knowledge discovery algorithms. Data mining is, however, not just a threat but also
an opportunity, a tool that can be used to promote worthy goals. These two perspectives on the ethical aspect of data mining were termed negative-ethical and positive-ethical, respectively, and discussed each in turn. Examples of both questionable and commendable uses of data mining were provided, and techniques available for protecting the privacy of targeted individuals were briefly reviewed. We found that many factors contribute to the overall ethical impact of a data mining project besides the actual content of the knowledge discovered; these include the intent of the data miner, the sensitivity of source data and the reliability and security of information. Partial answers to such fundamental questions as why privacy is considered important and when an action can be said to promote good were sought in classical and contemporary philosophy. Two generic schemes for analysing the ethical impact of computing products were presented and assessed in the context of knowledge discovery. The motivation of this work is to help practitioners and researchers of data mining attain a less obscure and more balanced image of the ethical issues - positive as well as negative ones - associated with this technology. Ignoring the issues will not make them disappear, so they should be addressed consciously, rationally and objectively.


REFERENCES

Abraham, T. and de Vel, O. (2002) Investigative Profiling with Computer Forensic Log Data and Association Rules, in Proc. 2002 IEEE International Conference on Data Mining, 11-18.
Adderley, R. and Musgrove, P.B. (2001) Data Mining Case Study: Modeling the Behavior of Offenders Who Commit Serious Sexual Assaults, in Proc. Seventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, 215-20.
Ali, K. and van Stam, W. (2004) TiVo: Making Show Recommendations Using a Distributed Collaborative Filtering Architecture, in Proc. 2004 ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining, 394-401.
Antal, P., Fannes, G., Timmerman, D., Moreau, Y. and De Moor, B. (2003) Bayesian applications of belief networks and multilayer perceptrons for ovarian tumor classification with rejection.
Artificial Intelligence in Medicine, 29, 39-60.
Bonchi, F., Giannotti, F., Mainetto, G. and Pedreschi, D. (1999) Using Data Mining Techniques in Fiscal Fraud Detection, in Proc. First International Conference on Data Warehousing and Knowledge Discovery, 369-76.
Brause, R., Langsdorf, T. and Hepp, M. (1999) Neural Data Mining for Credit Card Fraud Detection, in Proc. 11th International Conference on Tools with Artificial Intelligence, 103-6.
Brown, D.E. and Oxford, R.B. (2001) Data Mining Time Series with Applications to Crime Analysis, in Proc. 2001 IEEE International Conference on Systems, Man and Cybernetics, 1453-8 vol. 3.
Brown, G. (1990) The Information Game. Ethical Issues in a Microchip World. Humanities Press International, London.
Chou, P.B., Grossman, E., Gunopulos, D. and Kamesam, P. (2000) Identifying Prospective Customers, in Proc. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 447-56.
Cox, K.C., Eick, S.G., Wills, G.J. and Brachman, R.J. (1997) Brief Application Description; Visual Data Mining: Recognizing Telephone Calling Fraud. Data Mining and Knowledge Discovery, 1, 225-31.
Donoho, S. (2004) Early Detection of Insider Trading in Option Markets, in Proc. 2004 ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, 420-9.
Fogg, B.J. (2003) Persuasive Technology: Using Computers to Change What We Think and Do.
Morgan Kaufmann, San Francisco.
Forgionne, G.A., Gangopadhyay, A. and Adya, M. (1999) Detecting Health Care Fraud Using Intelligent Data Mining, in Proc. 1999 Information Resources Management Association International Conference, 801-5.
Gersten, W., Wirth, R. and Arndt, D. (2000) Predictive Modeling in Automotive Direct Marketing: Tools, Experiences and Open Issues, in Proc. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 398-406.
Han, S.-J. and Cho, S.-B. (2003) Detecting intrusion with rule-based integration of multiple models.
Computers & Security, 22, 613-23.
Hobbes, T. (1651) Leviathan.
Hayry, H. (1998) Individual Liberty and Medical Control. Ashgate Publishing, Aldershot.
Jensen, D., Rattigan, M. and Blau. H. (2003) Information Awareness: A Prospective Technical Assessment, in Proc. Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 378-87.
Kangas, L.J., Terrones, K.M., Keppel, R.D. and La Moria, R.D. (1999) Computer-Aided Tracking and Characterization of Homicides and Sexual Assaults (CATCH), in Applications and Science of Computational Intelligence II (Proceedings of SPIE vol. 3722), 250-60.
Kekes, J. (1993) The Morality of Pluralism. Princeton University Press, Princeton.
Kim, W., Agrawal, R., Faloutsos, C., Fayyad, U., Han, J., Piatetsky-Shapiro, G., Pregibon, D. and Uthurusamy, R. (2003) "Data Mining" Is NOT Against Civil Liberties.
http://www.acm.org/sigkdd/civil-liberties.pdf, referenced Mar 2, 2004.
Li, L., Tang, H., Wu, Z., Gong, J., Gruidl, M., Zou, J., Tockman, M. and Clark, R.A. (2004) Data mining techniques for cancer detection using serum proteomic profiling. Artificial Intelligence in Medicine, 32, 71-83.
Lin, S. and Brown, D.E. (2002) Outlier Detection and Data Association for Data Mining Criminal Incidents, in Proc. Third International Conference on Data Mining, 125-34.
Locke, J. (1698) Two Treatises of Government.
Lukas, L., Devos, A., Suykens, J.A.K., Vanhamme, L., Howe, F.A., Majos, C., Moreno-Torres, A., Van Der Graaf, M., Tate, A.R., Arus, C. and Van Huffel, S. (2004) Brain tumor classification based on long echo proton MRS signals. Artificial Intelligence in Medicine, 31, 73-89.
Lyon, D. (2001). Surveillance society. Monitoring everyday life. Open University Press, Buckingham.
Mill, J.S. (1859) On Liberty.
Moore, G.E. (1903) Principia Ethica. Cambridge University Press, Cambridge.
Mor, E. and Minguillon, J. (2004) E-learning Personalization Based on Itineraries and Long-term Navigational Behavior, in Proc. 13th International World Wide Web Conference, Alternate Track Papers & Posters, 264-5.
Nasraoui, O. and Petenes, C. (2004) An Intelligent Web Recommendation Engine Based on Fuzzy Approximate Reasoning, in Proc. 12th IEEE International Conference on Fuzzy Systems, 1116-21.
Nielsen, F.A. and Hansen, L.K. (2004) Finding related functional neuroimaging volumes. Artificial Intelligence in Medicine, 30, 141-51.
Nong, Y. and Xiangyang, L. (2000) Application of Decision Tree Classifiers to Computer Intrusion Detection, in Proc. Second International Conference on Data Mining, 381-90.
Oatley, G.C. and Ewart, B.W. (2003) Crimes analysis software: 'pins in maps', clustering and Bayes net prediction. Expert Systems with Applications, 25, 569-88.
Oh, S.H. and Lee, W.S. (2003) An anomaly intrusion detection method by clustering normal user behavior. Computers & Security, 22, 596-612.
Petrakis, E.G.M., Faloutsos, C. and Lin, K.-I. (2002) ImageMap: An Image Indexing Method Based on Spatial Similarity. IEEE Transactions on Knowledge and Data Engineering, 14, 979-87.
Rawls, J. (1971) A Theory of Justice. Harvard University Press, Cambridge, MA.
Richard, N., Dojat, M. and Garbay, C. (2004) Automated segmentation of human brain MR images using a multi-agent approach. Artificial Intelligence in Medicine, 30, 153-75.
Russell, S. and Lodwick, W. (1999) Fuzzy Clustering in Data Mining for Telco Database Marketing Campaigns, in Proc. 18th International Conference of the North American Fuzzy Information Processing Society, 720-6.
Seroussi, B, and Bouaud, J. (2003) Using OncoDoc as a computer-based eligibility screening system to improve accrual onto breast cancer clinical trials. Artificial Intelligence in Medicine, 29, 153-67.
Shao, H., Zhao, H. and Chang, G.-R. (2002) Applying Data Mining to Detect Fraud Behavior in Customs Declaration, in Proc. First International Conference on Machine Learning and Cybernetics, 1241-4.
Sokol, L. (1998) Using Data Mining to Support Health Care Fraud Detection, in Proc. Second International Conference on the Practical Application of Knowledge Discovery and Data Mining, 75-82.
Sorace, J.M. and Zhan, M. (2003) A data review and re-assessment of ovarian cancer serum proteomic profiling. BMC Bioinformatics, 4(24).
Syeda, M., Zhang, Y.-Q. and Pan, Y. (2002). Parallel Granular Neural Networks for Fast Credit Card Fraud Detection, in Proc. 2002 IEEE International Conference on Fuzzy Systems, 572-7.
Thuraisingham, B.M. and Ceruti, M.G. (2000) Understanding Data Mining and Applying It to Command, Control, Communications and Intelligence Environments, in Proc. 24th Annual International Computer Software and Applications Conference, 171-5.
Underson, L.F.G. (2002) Using Data Mining and Judgment Analysis to Construct a Predictive Model of Crime, in Proc. 2002 IEEE International Conference on Systems, Man and Cybernetics, 5 pp. vol. 7.
Verykios, V.S., Bertino, E., Fovino, I.N., Provenza, L.P., Saygin, Y. and Theodoridis, Y. (2004) State-of-the-art in Privacy Preserving Data Mining. ACM SIGMOD Record, 33, 50-57.
Viaene, S., Derrig, R.A. and Dedene, G. (2004) A Case Study of Applying Boosting Naive Bayes to Claim Fraud Diagnosis. IEEE Transactions on Knowledge and Data Engineering, 16, 612-20.
Wahlstrom, K. and Roddick, J.F. (2000) On the Impact of Knowledge Discovery and Data Mining, in Selected Papers from the Second Australian Institute Conference on Computer Ethics -
Volume 1, 22-27.
Walker, P.R., Smith, B., Liu, Q.Y., Famili, A.F., Valdes, J.J., Liu, Z. and Lach, B. (2004) Data mining of gene expression changes in Alzheimer brain. Artificial Intelligence in Medicine, 31, 137-54.
Wang, D.-W., Liau, C.-J. and Hsu, T.-s. (2004) Medical privacy protection based on granular computing. Artificial Intelligence in Medicine, 32, 137-49.
Wyns, B., Sette, S., Boullart, L., Baeten, D., Hoffman, I.E.A. and De Keyser, F. (2004) Prediction of diagnosis in patients with early arthritis using a combined Kohonen mapping and instance-based evaluation criterion. Artificial Intelligence in Medicine, 31, 45-55.
Zaffalon, M., Wesnes, K. and Petrini, O. (2003) Reliable diagnoses of dementia by the naive credal classifier inferred from incomplete cognitive data. Artificial Intelligence in Medicine, 29, 61-79.
Zhang, D.-Q.and Chen, S.-C. (2004) A novel kernelized fuzzy C-means algorithm with application in medical image segmentation. Artificial Intelligence in Medicine, 32, 37-50.
Zhang, Z., Salerno, J.J. and Yu, P.S. (2003) Applying Data Mining in Investigating Money Laundering Crimes, in Proc. Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 747-52.
Lauri Tuovinen (1979) received his Master of Science degree in information processing science in 2005 at the University of Oulu, Finland, with minor grades in mathematics and philosophy. He is currently a researcher in the Intelligent Systems Group (ISG), of which he has been a member since 2002, at the university's Department of Electrical and Information Engineering. He has previously studied software engineering and database management problems associated with the use of data mining in computational quality assurance of resistance spot welding joints. He also holds a nonacademic degree in music and is an amateur keyboard player of modest renown.
Juha Roning (1957) obtained the degree of Doctor of Technology in 1992 at the University of Oulu.
Since 1983 he has been a member of faculty at the University of Oulu, where he is currently Professor of Embedded Systems, head of the Department of Electrical and Information Engineering and principal investigator of ISG. He has been a visiting research scientist at the University of Cincinnati and held a Young Researcher Position in the Finnish Academy. Professor Roning has two patents and more than 150 publications; his main research interests are intelligent systems, especially mobile robots, machine vision, and software security.


